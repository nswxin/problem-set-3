---
title: "Problem Set 3"
author: "Pete Cuppernull"
date: "2/5/2020"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(tidymodels)
library(rsample)
library(glmnet)
library(leaps)
library(rcfss)
library(patchwork)
library(caret)
```

#Conceptual Exercises

##Generate Data 
```{r}
create_data <- function(){
  rnorm(1000, 3, 2)
}

data <- data.frame(replicate(20, create_data()))
betas <- sample(-500:500, 20, replace = TRUE)/100
betas[3] <- 0
betas[9] <- 0
betas[14] <- 0
betas[19] <- 0


error <- rnorm(1000, 0, 1)
data <- cbind(data, error)



data_scalar <- data %>%
  mutate(Y = X1*betas[1] + X2*betas[2] + X3*betas[3] + 
             X4*betas[4] + X5*betas[5] + X6*betas[6] + 
             X7*betas[7] + X8*betas[8] + X9*betas[9] + 
             X10*betas[10] + X11*betas[11] + X12*betas[12] + 
             X13*betas[13] + X14*betas[14] + X15*betas[15] + 
             X16*betas[16] + X17*betas[17] + X18*betas[18] + 
             X19*betas[19] + X20*betas[20])


```

##Split Data
```{r}
split <- initial_split(data_scalar, prop = .1) 
train <- training(split)
test <- testing(split)
```

##Best Subset Selection
```{r}
#do best subset
best_sub <- regsubsets(Y ~ ., 
                          data = train, 
                          nvmax = 20 
                          )

##Calculate MSE
train_mse <- rep(NA,20)
train_matrix <- model.matrix(Y ~., data=train)

get_mse <- function(model){
        coefi = coef(best_sub, id=model)
        pred = train_matrix[,names(coefi)]%*%coefi
        train_mse = mean((train$Y-pred)^2)
        return(train_mse)
}

mse <- map_dbl(1:20, get_mse)

train_mses <- as.data.frame(cbind(1:20, mse))


training_mse_plot <- ggplot(train_mses, aes(V1, mse)) +
  geom_line() +
  geom_point() +
  #geom_vline(xintercept = which.min(data_clean_cv_train$.estimate), linetype = 2) +
  labs(title = "Subset selection",
       x = "Number of variables",
       y = "MSE")

training_mse_plot
glimpse(best_sub)
coef(best_sub, id=2)
glimpse(best_sub$which)
        
str(train)
```

The model has the lowest MSE for the training set with 16 predictors.

##Plot the Test MSE for the best model of each size 
```{r}
##create vector of x's for each model size
bestsub_df <- as.data.frame(summary(best_sub)$which) %>%
  select(X1:X20)
#test model string maker
paste(paste("X", which(bestsub_df[2,-1] == TRUE), sep=""), collapse = " + ")
#nmake function, loop it, save models
model_cols <- function(number){
  model <- paste("Y ~", paste(paste("X", which(bestsub_df[number,-1] == TRUE), sep=""), collapse = " + "), sep = "")
}
best_models <- map(1:20, model_cols)
##create looped function that take model string and runs it on the test data
test_models <- function(predictors){
  model <- lm(predictors, data = train)
}
summary(test_models(best_models[[3]]))
##create MSE function

get_mse_test <- function(x){
      model <- test_models(best_models[[x]])
      Y_pred <- predict(model, newdata = test)

      mse_test <- mean((test$Y-Y_pred)^2)
      mse_test
}

##extract mse's and plot them on y axis against model size on x axis
mse_test <- map_dbl(1:20, get_mse_test)

test_mses <- as.data.frame(cbind(1:20, mse_test))

ggplot(test_mses, aes(V1, mse)) +
  geom_line() +
  geom_point() +
  #geom_vline(xintercept = which.min(data_clean_cv_train$.estimate), linetype = 2) +
  labs(title = "Subset selection",
       x = "Number of variables",
       y = "MSE")

test_mses
train_mses

```

##Q7
```{r}
beta_df <- as.data.frame(cbind(paste("X", 1:20, sep = ""), betas))

q7_function <- function(number){
  best_model_coefs <- rownames_to_column(as.data.frame(coef(best_sub, id=number))) 
  
  names(best_model_coefs) <- c("rowname", "coef")

  select_betas <- left_join(best_model_coefs, beta_df, by = c("rowname" = "V1")) %>%
                       na.omit()

  final <- select_betas %>%
              mutate(outcome = (as.numeric(as.character(betas)) - coef)^2) %>%
              summarize(sum_outcome = sum(outcome))
  
    final
}

q7_outcomes <- map_dfr(1:20, q7_function)

q7 <- as.data.frame(cbind(1:20, q7_outcomes))

ggplot(q7, aes(`1:20`, sum_outcome)) +
  geom_line() +
  geom_point() +
  #geom_vline(xintercept = which.min(data_clean_cv_train$.estimate), linetype = 2) +
  labs(title = "Subset selection",
       x = "Number of variables",
       y = "Sum of Beta Differences")
```


#Application Exercises
###Import Data
```{r}
gss_test <- read_csv("data/gss_test.csv")
gss_train <- read_csv("data/gss_train.csv")
```

##LS Linear Model
```{r}
lm_gss <- lm(egalit_scale ~ ., data = gss_train)

summary(lm_gss)
lm_gss_test <- predict(lm_gss, 
                newdata = gss_test)

lm_mse <- mse(gss_test, gss_test$egalit_scale, lm_gss_test)$.estimate
mse
```

The MSE of the test set for the least squares model is `r lm_mse`.

##Ridge Regression
```{r}
gss_train_x <- model.matrix(egalit_scale ~ ., gss_train)[, -1]
gss_train_y <- log(gss_train$egalit_scale)

gss_train_x <- model.matrix(egalit_scale ~ ., gss_test)[, -1]
gss_train_y <- log(gss_test$egalit_scale)
# First: ridge regression
gss_ridge <- glmnet(
  x = gss_train_x,
  y = gss_train_y,
  alpha = 0
)

plot(gss_ridge, xvar = "lambda")


# Tuning lambda
# Apply CV Ridge regression to ames data
gss_ridge_cv <- cv.glmnet(
  x = gss_train_x,
  y = gss_train_y,
  alpha = 0
)
lambda_min_ridge <- which(gss_ridge_cv$lambda == gss_ridge_cv$lambda.min)
ridge_mse <- gss_ridge_cv$cvm[lambda_min_ridge]
```

The MSE of the test set for the Ridge regression model is `r ridge_mse`.

## Lasso Regression
```{r}

# First: ridge regression
gss_lasso <- glmnet(
  x = gss_train_x,
  y = gss_train_y,
  alpha = 1
)

plot(gss_lasso, xvar = "lambda")


# Tuning lambda
# Apply CV Ridge regression to ames data
gss_lasso_cv <- cv.glmnet(
  x = gss_train_x,
  y = gss_train_y,
  alpha = 1
)

lambda_min_lasso <- which(gss_lasso_cv$lambda == gss_lasso_cv$lambda.min)
lasso_mse <- gss_lasso_cv$cvm[lambda_min_lasso]
```

The MSE of the test set for the Ridge regression model is `r lasso_mse` and there are 18 non-zero coefficients.

## Elastic Net

```{r}
lasso    <- glmnet(gss_train_x, gss_train_y, alpha = 1.0) 
elastic1 <- glmnet(gss_train_x, gss_train_y, alpha = 0.25) 
elastic2 <- glmnet(gss_train_x, gss_train_y, alpha = 0.75) 
ridge    <- glmnet(gss_train_x, gss_train_y, alpha = 0.0)

fold_id <- sample(1:10, size = length(gss_train_y), replace = TRUE)

tuning_grid <- tibble::tibble(
  alpha      = seq(0, 1, by = .1),
  mse_min    = NA,
  mse_1se    = NA,
  lambda_min = NA,
  lambda_1se = NA
)

for(i in seq_along(tuning_grid$alpha)) {
  # fit CV model for each alpha value
  fit <- cv.glmnet(gss_train_x, 
                   gss_train_y, 
                   alpha = tuning_grid$alpha[i], 
                   foldid = fold_id)
  
  # extract MSE and lambda values
  tuning_grid$mse_min[i]    <- fit$cvm[fit$lambda == fit$lambda.min]
  tuning_grid$mse_1se[i]    <- fit$cvm[fit$lambda == fit$lambda.1se]
  tuning_grid$lambda_min[i] <- fit$lambda.min
  tuning_grid$lambda_1se[i] <- fit$lambda.1se
}

tuning_grid %>%
  mutate(se = mse_1se - mse_min) %>%
  ggplot(aes(alpha, mse_min)) +
  geom_line(size = 2) +
  geom_ribbon(aes(ymax = mse_min + se, ymin = mse_min - se), alpha = .25) +
  ggtitle("MSE Â± one standard error")

elastic_mse <- min(tuning_grid$mse_min)

##Non zero coefs
cv.glmnet(
  x = gss_train_x,
  y = gss_train_y,
  alpha = .4
)

```

The combination of $\alpha$ and $\lambda$ thatproduce the lwoest cross validation MSE are 0.4 and 0.102, respectively. The test MSE is `r elastic_mse` and there are 20 nonzero coefficients.